use rstructor::{LLMModel, SchemaType};
use serde::{Deserialize, Serialize};

#[derive(LLMModel, Serialize, Deserialize)]
struct WeatherInfo {
    #[llm(description = "City name to get weather for")]
    city: String,
    
    #[llm(description = "Current temperature in Celsius", example = 22.5)]
    temperature: f32,
    
    #[llm(description = "Weather description")]
    description: Option<String>,
}

fn main() -> rstructor::Result<()> {
    // Get the schema generated by the derive macro
    let schema = WeatherInfo::schema();
    
    println!("Weather Info Schema (auto-generated by LLMModel derive):");
    println!("{}", serde_json::to_string_pretty(schema.to_json()).unwrap());
    
    // Sample usage with OpenAI (uncomment to run with your API key)
    /*
    // Create an OpenAI client
    let client = OpenAIClient::new("your-api-key-here")?
        .model(OpenAIModel::Gpt35Turbo)
        .temperature(0.0)
        .build();
    
    // Define a prompt
    let prompt = "What's the weather like in Paris right now?";
    
    // Call the LLM to get a structured output
    let weather: WeatherInfo = client.generate_struct(prompt).await?;
    
    // Use the structured result
    println!("Weather for {}: {} Â°C", weather.city, weather.temperature);
    if let Some(desc) = weather.description {
        println!("Description: {}", desc);
    }
    */
    
    // Create a sample instance
    let weather = WeatherInfo {
        city: "Paris".to_string(),
        temperature: 25.5,
        description: Some("Sunny".to_string()),
    };
    
    println!("\nSample Weather Info:");
    println!("{}", serde_json::to_string_pretty(&weather).unwrap());
    
    Ok(())
}